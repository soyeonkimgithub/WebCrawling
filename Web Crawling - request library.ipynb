{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Crawling \n",
    "\n",
    "- Sydney Weather\n",
    "- https://weather.com/weather/today/l/e2a77b6dabb86cf1bf18990058242f520c3490682bf751655fcad2a6c166e93f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/soyeonkim/opt/anaconda3/lib/python3.8/site-packages (2.27.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/soyeonkim/opt/anaconda3/lib/python3.8/site-packages (4.9.1)\n",
      "Requirement already satisfied: lxml in /Users/soyeonkim/opt/anaconda3/lib/python3.8/site-packages (4.5.2)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /Users/soyeonkim/opt/anaconda3/lib/python3.8/site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /Users/soyeonkim/opt/anaconda3/lib/python3.8/site-packages (from requests) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/soyeonkim/opt/anaconda3/lib/python3.8/site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/soyeonkim/opt/anaconda3/lib/python3.8/site-packages (from requests) (1.25.9)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/soyeonkim/opt/anaconda3/lib/python3.8/site-packages (from beautifulsoup4) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "URL = \"https://weather.com/weather/today/l/e2a77b6dabb86cf1bf18990058242f520c3490682bf751655fcad2a6c166e93f\"\n",
    "resp = requests.get(URL)\n",
    "print(resp.status_code)\n",
    "#print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the textual response and get the HTML of the web page. \n",
    "depending on how we want to specify the data, there are two ways, \n",
    "1. consider the HTML as a kind of XML document and use the XPath language to extract the element\n",
    "\n",
    "2. use CSS selectors on the HTML document, which we can make use of the BeautifulSoup library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "67Â°\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "if resp.status_code == 200:\n",
    "    # using lxml\n",
    "    dom = etree.HTML(resp.text)\n",
    "    elements = dom.xpath(\"//span[@data-testid='TemperatureValue' and contains(@class,'CurrentConditions')]\")\n",
    "    print(elements[0].text)\n",
    "    \n",
    "    # using BeautifulSoup\n",
    "    soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "    elements = soup.select('span[data-testid=\"TemperatureValue\"][class^=\"CurrentConditions\"]')\n",
    "    print(elements[0].text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            DATE T10YIE\n",
      "0     2017-04-17   1.88\n",
      "1     2017-04-18   1.85\n",
      "2     2017-04-19   1.85\n",
      "3     2017-04-20   1.85\n",
      "4     2017-04-21   1.84\n",
      "...          ...    ...\n",
      "1299  2022-04-08   2.87\n",
      "1300  2022-04-11   2.91\n",
      "1301  2022-04-12   2.86\n",
      "1302  2022-04-13    2.8\n",
      "1303  2022-04-14   2.89\n",
      "\n",
      "[1304 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    " \n",
    "URL = \"https://fred.stlouisfed.org/graph/fredgraph.csv?id=T10YIE&cosd=2017-04-14&coed=2022-04-14\"\n",
    "resp = requests.get(URL)\n",
    "\n",
    "if resp.status_code == 200:\n",
    "    csvtext = resp.text\n",
    "    csvbuffer = io.StringIO(csvtext)\n",
    "    df = pd.read_csv(csvbuffer)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON data, convert to dictionary    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'login': 'jbrownlee', 'id': 12891, 'node_id': 'MDQ6VXNlcjEyODkx', 'avatar_url': 'https://avatars.githubusercontent.com/u/12891?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/jbrownlee', 'html_url': 'https://github.com/jbrownlee', 'followers_url': 'https://api.github.com/users/jbrownlee/followers', 'following_url': 'https://api.github.com/users/jbrownlee/following{/other_user}', 'gists_url': 'https://api.github.com/users/jbrownlee/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/jbrownlee/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/jbrownlee/subscriptions', 'organizations_url': 'https://api.github.com/users/jbrownlee/orgs', 'repos_url': 'https://api.github.com/users/jbrownlee/repos', 'events_url': 'https://api.github.com/users/jbrownlee/events{/privacy}', 'received_events_url': 'https://api.github.com/users/jbrownlee/received_events', 'type': 'User', 'site_admin': False, 'name': 'Machine Learning Mastery', 'company': 'Machine Learning Mastery', 'blog': 'http://MachineLearningMastery.com', 'location': None, 'email': None, 'hireable': None, 'bio': 'Making developers awesome at machine learning.', 'twitter_username': None, 'public_repos': 6, 'public_gists': 0, 'followers': 2061, 'following': 1, 'created_at': '2008-06-07T02:20:58Z', 'updated_at': '2023-07-19T16:19:13Z'}\n"
     ]
    }
   ],
   "source": [
    "URL = \"https://api.github.com/users/jbrownlee\"\n",
    "resp = requests.get(URL)\n",
    "if resp.status_code == 200:\n",
    "    data = resp.json()\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary data (ZIP file or image)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://en.wikipedia.org/static/images/project-logos/enwiki.png\"\n",
    "wikilogo = requests.get(URL)\n",
    "if wikilogo.status_code == 200:\n",
    "    with open(\"enwiki.png\", \"wb\") as fp:\n",
    "        fp.write(wikilogo.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
